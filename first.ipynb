{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f55c2a9d-2584-4bd6-8c64-d04f0be2a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # For numerical analysis and work\n",
    "import pandas as pd # For summarizing the code and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9982b11-bb14-49b9-a357-f61cfcdaa053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer:\n",
    "    def __init__(self):\n",
    "        self.inputs = None\n",
    "        self.output = None\n",
    "        \n",
    "        self.d_inputs = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ...\n",
    "\n",
    "    def backward(self, d_values):\n",
    "        ...\n",
    "\n",
    "    def __call__(self, inputs, backward_pass=False):\n",
    "        if backward_pass:\n",
    "            self.backward(inputs)\n",
    "        else:\n",
    "            self.forward(inputs)\n",
    "        \n",
    "class Dense(BaseLayer):\n",
    "    def __init__(self, d_out, d_in=None, keepbias=True, activation='linear'):\n",
    "        self.d_out = d_out\n",
    "        self.d_in = d_in\n",
    "        \n",
    "        self.keepbias = keepbias\n",
    "        self.activation = activation\n",
    "        \n",
    "        if self.d_in:\n",
    "            self.__init_params_(self.d_in)\n",
    "\n",
    "    def __init_params_(self, d_in):\n",
    "        self.d_in = d_in\n",
    "        self.weights = np.random.randn(self.d_in, self.d_out) / np.sqrt(d_in)\n",
    "        if self.keepbias:\n",
    "            self.biases = np.random.randn(self.d_out)\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.biases] if self.keepbias else [self.weights]\n",
    "\n",
    "    @parameters.setter\n",
    "    def parameters(self, value):\n",
    "        self.weights = value[0]\n",
    "        if self.keepbias:\n",
    "            self.biases = value[1]\n",
    "\n",
    "    @property\n",
    "    def d_parameters(self):\n",
    "        return [self.d_weights, self.d_biases] if self.keepbias else [self.d_weights]\n",
    "\n",
    "    @d_parameters.setter\n",
    "    def d_parameters(self, value):\n",
    "        self.d_weights = value[0]\n",
    "        if self.keepbias:\n",
    "            self.d_biases = value[1]\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs @ self.weights + self.biases\n",
    "\n",
    "    def backward(self, d_values):\n",
    "        n = len(d_values.shape)\n",
    "        self.d_weights = self.inputs.transpose((*range(n-2), n-1, n-2)) @ d_values\n",
    "        if self.keepbias:\n",
    "            self.d_biases = np.sum(self.inputs, axis=range(n-1))\n",
    "\n",
    "        self.d_inputs = d_values @ self.weights.T\n",
    "\n",
    "    def summary(self):\n",
    "        {\n",
    "            \"name\": \"Dense\",\n",
    "            \"n_params\": self._n_params,\n",
    "            \"activation\": self.activation,\n",
    "            \"keepbias\": self.keepbias,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _n_params(self):\n",
    "        return self.d_out*self.d_in+self.d_out if self.keepbias else self.d_out*self.d_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "731452c3-c270-4dfb-9cc5-38aedc99de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(BaseLayer):\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(inputs, 0)\n",
    "\n",
    "    def backward(self, d_values):\n",
    "        self.d_inputs = np.where(self.inputs>0, 1, 0)*d_values\n",
    "\n",
    "class Linear(BaseLayer):\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = self.inputs\n",
    "\n",
    "    def backward(self, d_values):\n",
    "        self.d_inputs = d_values\n",
    "\n",
    "class LeakyReLU(BaseLayer):\n",
    "    def __init__(self, leaky=0.001):\n",
    "        super().__init__()\n",
    "        self.leaky_constant = leaky\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(inputs, inputs*self.leaky_constant)\n",
    "\n",
    "    def backward(self, d_values):\n",
    "        self.d_inputs = np.where(self.inputs>0, 1, self.leaky_constant)*d_values\n",
    "\n",
    "class Sigmoid(BaseLayer):\n",
    "    epsilon = 1e-8\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1/(1+np.exp(-inputs)+self.epsilon)\n",
    "\n",
    "    def backward(self, d_values):\n",
    "        self.d_inputs = (self.output*(1-self.output))*d_values\n",
    "        \n",
    "class TanH(BaseLayer):\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.tanh(inputs)\n",
    "\n",
    "    def backward(self, d_values):\n",
    "        self.d_inputs = (1-self.output*self.output)*d_values\n",
    "\n",
    "class SoftMax(BaseLayer):\n",
    "    def __init__(self, loss_c=True):\n",
    "        super().__init__()\n",
    "        self.loss_c = loss_c\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        n = len(inputs.shape)\n",
    "        max_vals = np.max(inputs, axis=n-1, keepdims=True)\n",
    "        exp_vals = np.exp(inputs-max_vals)\n",
    "        self.output = np.sum(exp_vals, axis=n-1, keepdims=True)\n",
    "\n",
    "    def backward(self, d_values):\n",
    "        if not self.loss_c:\n",
    "            output_shape = d_values.shape\n",
    "            d_out = output_shape[-1]\n",
    "            M = np.tile(self.output, d_out).reshape(*output_shape, d_out)\n",
    "            I = np.eye(d_out)\n",
    "            Z = M * (I - M.T)\n",
    "    \n",
    "            self.output = np.sum(np.multiply(Z, d_values[..., np.newaxis, :]), axis=-1)\n",
    "        else:\n",
    "            self.output = d_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17ec3212-9ca1-4ad7-a604-ccb8c7642d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_parser(activation, *args, **kwargs):\n",
    "    match activation:\n",
    "        case \"relu\": activator = ReLU\n",
    "        case \"linear\": activator = Linear\n",
    "        case \"tanh\": activator = TanH\n",
    "        case \"sigmoid\": activator = Sigmoid\n",
    "        case \"leaky-relu\": activator = LeakyReLU\n",
    "        case \"softmax\": activator = SoftMax\n",
    "\n",
    "    return activator(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89006358-c116-4bf1-bc89-36f0a4dac5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, d_in, hidden_state, call_state, d_out):\n",
    "        self.dn1 = Dense(d_out=call_state, d_in=d_in)\n",
    "        self.dh1 = Dense(d_out=call_state, d_in=hidden_state)\n",
    "\n",
    "        self.dn2 = Dense(d_out=call_state, d_in=d_in)\n",
    "        self.dh2 = Dense(d_out=call_state, d_in=hidden_state)\n",
    "        \n",
    "        self.dn1 = Dense(d_out=call_state, d_in=d_in)\n",
    "        self.dh1 = Dense(d_out=call_state, d_in=hidden_state)\n",
    "\n",
    "        self.dn1 = Dense(d_out=call_state, d_in=d_in)\n",
    "        self.dh1 = Dense(d_out=call_state, d_in=hidden_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
